{"categories":[{"title":"algo","uri":"https://foxisawesome.github.io/categories/algo/"},{"title":"ML","uri":"https://foxisawesome.github.io/categories/ml/"},{"title":"note","uri":"https://foxisawesome.github.io/categories/note/"},{"title":"sql","uri":"https://foxisawesome.github.io/categories/sql/"}],"posts":[{"content":"This post mainly talks about how to implement ordinal logist regression prediction in numpy using matrix form.\nLoad libaries # libs import pandas as pd import numpy as np import time import os # plotting setting from matplotlib import pyplot as plt %matplotlib inline import seaborn as sns sns.set_style('darkgrid', {'axes.facecolor': '.9'}) sns.set_palette(palette='deep') sns_c = sns.color_palette(palette='deep') # display setting from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = \u0026quot;all\u0026quot; from IPython.display import display np.set_printoptions(precision=12)  Algo: The ordinal logistic regression is a variant of normal logistic regression, where its numerator is CDF. It garuantees that the output prob of each category is rank ordered.\n$$ \\begin{eqnarray} P(k) \u0026amp;=\u0026amp; P(x \u0026gt;= k) \\\\\\\nP(k-1) \u0026amp;=\u0026amp; P(x \u0026gt;= k-1) - P(x \u0026gt;= k) \\\\\\\n\\cdots \\\\\\\nP(1) \u0026amp;=\u0026amp; 1- P(target \u0026gt;= 2) \\\\\\\n\\end{eqnarray} $$\n#tape x: n x p (n rows, p features) n = 1000 p = 5 # x: n*p, first column is intercept x = np.random.uniform(0,1,(n,p)) x[:,0] = 1 # weight: from 3-8, to 4-8; where to_3 is the basis w = np.random.uniform(0,1,(30,p)) # last 5 rows are from_8, p88=1, p8x=0, b/c diff, need to set all cdf 1. w[-5:,:] = np.Inf # simulate p38 = 0 w[0,:] = np.NINF z = np.dot(x,w.T) y = 1 / (1+ np.exp(-z)) y = y.reshape(n,6,5) # stacking transition matrix # tran.shape: n x 6 x 6 # tran_M: ith row: # [p38, p37, p36, p35, p34, p33] # ... # [p88, p87, p86, p85, p84, p83] # prob_x8 = y[0], prob_x7 = y[1]-y[0], ..., prob_x4=y[-2]-y[-1], prob_x3 = 1-y[-1] p_f8 = y[:,:,0].reshape(n,-1,1) p_f74 = np.diff(y[:,:,:]) p_f3 = (1- y[:,:,-1]).reshape(n,-1,1) tran_M = np.dstack((p_f8, p_f74, p_f3)) # tran_M[0,:,:] # check row add up to 1 # np.sum(tran_M[0,0,:]) # s_t0: n x 1 x 6, each row prob add to 1 # s_t0 = ith row: [s3, s4, s5, s6, s7, s8] s_t0 = np.random.uniform(0,1,(n,1,6)) s_t0 /= np.sum(s_t0, axis=2).reshape(n,1,1) # check: np.sum(s_t0, axis=2)==1 # s_t1: ith row: [s8, s7, s6, s5, s4, s3] s_t1 = np.einsum('iab,ibd-\u0026gt;iad', s_t0, tran_M) # s_t0[0,:,:] # tran_M[0,:,:] # S_t1[0,:,:] # check: np.sum(pd,axis=2)==1 # speed: 1MM 750ms; 30 x 1MM out = s_t1.reshape(n,6) # n:1MM; p=5, 750ms  reference  https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c https://towardsdatascience.com/deep-ordinal-logistic-regression-1afd0645e591  3d-array multiply 2d # a: n*6*6 a = np.empty((0,6,6)) for i in range(3): a = np.vstack((a, np.ones((1,6,6))*(i+1))) # a = np.array([ # [[1,1,1,1,1,1],[1,1,1,1,1,1],[1,1,1,1,1,1],[1,1,1,1,1,1],[1,1,1,1,1,1],[1,1,1,1,1,1]], # [[2,2,2,2,2,2],[2,2,2,2,2,2],[2,2,2,2,2,2],[2,2,2,2,2,2],[2,2,2,2,2,2],[2,2,2,2,2,2]], # [[3,3,3,3,3,3],[3,3,3,3,3,3],[3,3,3,3,3,3],[3,3,3,3,3,3],[3,3,3,3,3,3],[3,3,3,3,3,3]] # ]) # b: 1*6 b = np.array([[1, 1, 1, 1, 1, 1]]) # results: n*1*6 np.tensordot(b,a,axes=((1),(1))).swapaxes(0,1)  array([[[ 6., 6., 6., 6., 6., 6.]], [[12., 12., 12., 12., 12., 12.]], [[18., 18., 18., 18., 18., 18.]]])  ","id":0,"section":"notes","summary":"This post mainly talks about how to implement ordinal logist regression prediction in numpy using matrix form.\nLoad libaries # libs import pandas as pd import numpy as np import time import os # plotting setting from matplotlib import pyplot as plt %matplotlib inline import seaborn as sns sns.set_style('darkgrid', {'axes.facecolor': '.9'}) sns.set_palette(palette='deep') sns_c = sns.color_palette(palette='deep') # display setting from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = \u0026quot;all\u0026quot; from IPython.display import display np.set_printoptions(precision=12)  Algo: The ordinal logistic regression is a variant of normal logistic regression, where its numerator is CDF.","tags":null,"title":"Ordinal Logit and Markov Chain Process Vectorization","uri":"https://foxisawesome.github.io/notes/ordinal_logit_vect/","year":"2020"},{"content":"Hypothesis Testing Basics  $\\alpha$: Type I error, false positive rate. Reject $H_0$ when $H_0$ is true. Significance level of test. (minimize $\\alpha$) $\\beta$: Type II error, false negative rate. Fail to reject $H_0$ when $H_0$ is false. power: $1-\\beta$, probablility of making correct decision if alternative hypothesis is true. (maximize power). or reject null when null is false. $p$-value: the probability that we observe a more extreme statistics when $H_0$ was true. The smallest significance level $\\alpha$ that leads to reject $H_0$. Here, the null hypothesis $H_0$ is that $p = p_0$, and the alternative hypothesis $H_a$ is that $p \u0026gt; p_0$: this is a one-sided test. In particular, we’ll use the value $p_a$ as the alternative value so that we can compute power. The null distribution is shown on the left, and an alternative distribution is shown on the right. The $\\alpha = 0.05$ threshold for the alternative hypothesis is shown as $p^*$. The power is the probability of making the correct decision when the alternative hypothesis is true. The probability of a Type I error (false positive) is shown in blue, the probability of a Type II error (false negative) is shown in red, and the power is shown in yellow and blue combined (it’s the area under the right curve minus the red part).   t-test Assumptions:\n The data are continuous (not discrete). The data follow the normal probability distribution. The variances of the two populations are equal. (If not, the Aspin-Welch Unequal-Variance test is used.) The two samples are independent. There is no relationship between the individuals in one sample as compared to the other. Both samples are simple random samples from their respective populations.  {\\frac {\\left(n_{1}-1\\right)s_{X_{1}}^{2}+\\left(n_{2}-1\\right)s_{X_{2}}^{2}}{}}\n$$ H_0: \\mu_{1} = \\mu_{2} $$\nTwo Sample t-test for Homogeneous Variances: Student\u0026rsquo;s t-test, pooled two-sample t-test:\n$$ t=\\frac{\\bar{X}_1 - \\bar{X}2}{s_p \\cdot \\sqrt{1/n_1 + 1/n_2}} \\qquad \\text{where: } s_p=\\sqrt{\\frac{(n_1-1){s^2}{X}}{n_1+n_2-2}} $$\nTwo-Sample t-test for Heterogeneous Variances: Welch\u0026rsquo;s t-test $$ \\displaystyle t={\\frac {{\\bar {X}}{1}-{\\bar {X}}{2}}{s_{\\bar {\\Delta }}}} \\qquad \\text{where: } \\displaystyle s_{\\bar {\\Delta }}={\\sqrt {{\\frac {s_{1}^{2}}{n_{1}}}+{\\frac {s_{2}^{2}}{n_{2}}}}} $$ Here $s_i^2$ is the unbiased estimator of the variance of each of the two samples with $n_i$ = number of participants in group *i* (1 or 2). In this case $s_{\\bar {\\Delta }}^2$ is not a pooled variance. For use in significance testing, the distribution of the test statistic is approximated as an ordinary Student\u0026rsquo;s *t*-distribution with the degrees of freedom calculated using: $$ \\displaystyle \\mathrm {d.f.} = {\\left({\\frac {s_{1}^{2}}{n_{1}}}+{\\frac {s_{2}^{2}}{n_{2}}}\\right)^{2} / \\left({{\\frac {\\left(s_{1}^{2}/n_{1}\\right)^{2}}{n_{1}-1}}+{\\frac {\\left(s_{2}^{2}/n_{2}\\right)^{2}}{n_{2}-1}}} \\right)} $$\nSample size $n$ formula Two-tail test sample size: $$ n = \\frac{\\sigma^2 (Z_{\\alpha/2} - Z_{\\beta})^2}{(\\mu_a - \\mu_0)^2} $$\n ","id":1,"section":"notes","summary":"Hypothesis Testing Basics  $\\alpha$: Type I error, false positive rate. Reject $H_0$ when $H_0$ is true. Significance level of test. (minimize $\\alpha$) $\\beta$: Type II error, false negative rate. Fail to reject $H_0$ when $H_0$ is false. power: $1-\\beta$, probablility of making correct decision if alternative hypothesis is true. (maximize power). or reject null when null is false. $p$-value: the probability that we observe a more extreme statistics when $H_0$ was true.","tags":null,"title":"Statistics Notes (Unfinished)","uri":"https://foxisawesome.github.io/notes/stat_notes/","year":"2020"},{"content":"Reference links  PSU: https://online.stat.psu.edu/stat510/  Why does a time series have to be stationary? Stationarity is a one type of dependence structure.\nSuppose we have a data $X_1,\u0026hellip;,X_n$. The most basic assumption is that $X_i$ are independent, i.e. we have a sample. The independence is a nice property, since using it we can derive a lot of useful results. The problem is that sometimes (or frequently, depending on the view) this property does not hold.\nNow independence is a unique property, two random variables can be independent only in one way, but they can be dependent in various ways. So stationarity is one way of modeling the dependence structure. It turns out that a lot of nice results which holds for independent random variables (law of large numbers, central limit theorem to name a few) hold for stationary random variables (we should strictly say sequences). And of course it turns out that a lot of data can be considered stationary, so the concept of stationarity is very important in modeling non-independent data.\nWhen we have determined that we have stationarity, naturally we want to model it. This is where ARMA models come in. It turns out that any stationary data can be approximated with stationary ARMA model, thanks to Wold decomposition theorem. So that is why ARMA models are very popular and that is why we need to make sure that the series is stationary to use these models.\nNow again the same story holds as with independence and dependence. Stationarity is defined uniquely, i.e. data is either stationary or not, so there is only way for data to be stationary, but lots of ways for it to be non-stationary. Again it turns out that a lot of data becomes stationary after certain transformation. ARIMA model is one model for non-stationarity. It assumes that the data becomes stationary after differencing.\nIn the regression context the stationarity is important since the same results which apply for independent data holds if the data is stationary.\nIn addition to this, stationary processes avoid the problem of spurious regression.\nStationary ARMA Processes Covariance-stationary (weakly stationary): $$ \\begin{eqnarray} \u0026amp;E(Y_t) = \\mu \\qquad \u0026amp;\\text{for all $t$} \\nonumber \\\\\\\n\u0026amp;E(Y_t-\\mu)(Y_{t-j} - \\mu) = \\gamma_j \\qquad \u0026amp;\\text{for all $t$ and any $j$} \\end{eqnarray} $$ where $\\gamma_j$ is the $j$-th autocovariance. **Not time varying**.\n For any covariance-stationary process: $\\gamma_j = \\gamma_{-j}$. $MA(1)$ process is covariance stationary. $AR(1)$ when $|\\phi|\u0026lt;1$ then it is covariance stationary. The stationarity of an $ARMA(p,q)$ process depends entirely on the autoregressive parameters and not on the moving average parameters.  Strict stationary: For any values of $j_1, j_2, \\cdots, j_n$, the joint distribution of $(Y_t, Y_{t+j_1}, Y_{t+j_2}, \\cdots, Y_{t+j_n})$ depends only on the intervals separating the dates $(j_1, j_2, \\cdots, j_n)$ and not on the date itself $(t)$, i.e.:$F(Y_0, Y_{j_1}, Y_{j_2}, \\cdots, Y_{j_n}) = F(Y_t, Y_{t+j_1}, Y_{t+j_2}, \\cdots, Y_{t+j_n})$, where $F(\\cdot)$ is the joint distribution function.\n If a process is strictly stationary with finite second moments, then it must be covariance-stationary. Autocorrelation: $\\rho_j \\equiv \\gamma_j / \\gamma_0 $  Ergodicity:   Ensemble Average: $E (Y_t) = \\underset{I \\rightarrow \\infty}{\\text{plim}} (1/I) \\sum_{i=1}^I Y_t^{(i)} $\n  Time Average: $ \\bar{y} \\equiv (1/T)\\sum^T_{t=1} y_t^{(1)}$\n  A covariance-stationary process is said to be \\textbf{ergodic} for the mean if time average converges in probability to $E(Y_t)$ as $T\\rightarrow \\infty$.\n  A process will be ergodic for the mean provided that the auto-covariance $\\gamma_j$ goes to zero sufficiently quickly as $j$ becomes large.\n  Invertibility: For the $MA(q)$ process: $ Y_t - \\mu = \\Phi(L)\\varepsilon_t$. Factor the moving average operator as: $\\Phi(L) = (1-\\lambda_1 L)(1-\\lambda_2 L)\\cdots(1-\\lambda_q L)$, if $|\\lambda_i|\u0026lt;1$ for all $i$, then the roots of $\\Phi(L)$ all lie outside of the unit circle and MA(q) is invertible. If $MA(p)$ is invertible then it has a $AR(\\infty)$ representation.\nSums of ARMA Processes:  $MA(1) + WN \\Rightarrow MA(1)$ $ MA(q_1) + MA(q_2) = MA(max[q_1, q_2])$ $AR(1) + AR(1) = ARMA(2,1) $  Wold\u0026rsquo;s Decomposition Theorem Any zero-mean covariance-stationary process $Y_t$ can be represented in the form: $$ Y_t = \\sum^\\infty_{j=0}\\Psi_j\\varepsilon_{t-j}+\\kappa_t $$ where $\\Psi_0 = 1$ and $\\Psi_j^2 \u0026lt; \\infty$. The term $\\varepsilon_t$ is white noise and represents the error made in forecasting $Y_t$ on the basis of a linear function of lagged $Y$: $\\varepsilon_t \\equiv Y_t - \\hat{E}(Y_t | Y_{t-1}, Y_{t-2}, \\cdots)$. The value of $\\kappa_t$ is uncorrelated with $\\varepsilon_{t-j}$ for any $j$, though $\\kappa_t$ can be predicted arbitrarily well from a linear function of past values of $Y$: $\\kappa_t = \\hat{E} (\\kappa_t | Y_{t-1}, Y_{t-2}, \\cdots). $\nBox-Jenkins Approach / Model Philosophy  If necessary, transform data, such that covariance stationarity is achieved. Inspect, Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) for initial guesses of p and q. (Decaying ACF indicates the the goodness of fit for AR($p$) part. Only the PACF could tell the lag order $q$. It should be up to the last PACF outside the 95% significance band) Estimate proposed model. Check residuals (diagnostic tests) and stationarity of process. The residuals should follow normal distribution without any serial correlation. Plot ACF, QQ-plot for visual test or Jarque-Bera, Shapiro-Bera test for more rigorous purposes. Also Box-Ljung test could be used to test the goodness of fit for the proposed $ARMA(p,q)$ model. If item 4 fails, go to item 2 and repeat. If in doubt, choose the more parsimonious model specification.  Identify The Orders of AR or MA Models   The partial autocorrelations at all lags can be computed by fitting a succession of autoregressive models with increasing numbers of lags. In particular, the partial autocorrelation at lag k is equal to the estimated AR(k) coefficient in an autoregressive model with k terms\u0026ndash;i.e., a multiple regression model in which Y is regressed on LAG(Y,1), LAG(Y,2), etc., up to LAG(Y,k). Thus, by mere inspection of the PACF you can determine how many AR terms you need to use to explain the autocorrelation pattern in a time series: if the partial autocorrelation is significant at lag k and not significant at any higher order lags\u0026ndash;i.e., if the PACF \u0026ldquo;cuts off\u0026rdquo; at lag k\u0026ndash;then this suggests that you should try fitting an autoregressive model of order k.\n  PACF determins $p$. $$ y_t = C + \\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + \\cdots + \\alpha_m y_{t-m} + u_t $$ If the PACF ($\\alpha_i$) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is **positive** i.e., if the series appears slightly \u0026ldquo;underdifferenced\u0026rdquo;\u0026ndash;then consider adding an **AR** term to the model. The lag at which the PACF cuts off is the indicated number of AR terms.\n  ACF determins $q$. $$ \\rho_h = \\frac{\\gamma_h}{\\gamma_0} $$ If the ACF($\\rho_h$) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is negative\u0026ndash;i.e., if the series appears slightly \u0026ldquo;overdifferenced\u0026rdquo;\u0026ndash;then consider adding an MA term to the model. The lag at which the ACF cuts off is the indicated number of MA terms.\n  A partial autocorrelation is the amount of correlation between a variable and a lag of itself that is not explained by correlations at all lower-order-lags. The autocorrelation of a time series $Y$ at lag 1 is the coefficient of correlation between $Y_{t}$ and $Y_{t-1}$, which is presumably also the correlation between $Y_{t-1}$ and $Y_{t-2}$. But if $Y_{t}$ is correlated with $Y_{t-1}$, and $Y_{t-1}$ is equally correlated with $Y_{t-2}$, then we should also expect to find correlation between $Y_{t}$ and $Y_{t-2}$. In fact, the amount of correlation we should expect at lag 2 is precisely the square of the lag-1 correlation. Thus, the correlation at lag 1 \u0026ldquo;propagates\u0026rdquo; to lag 2 and presumably to higher-order lags. The partial autocorrelation at lag 2 is therefore the difference between the actual correlation at lag 2 and the expected correlation due to the propagation of correlation at lag 1.\n  Models of Nonstationary Time Series If the raw time series data is nonstationary, two popular approaches exist:\n  Trend-stationary model: $y_t = \\alpha + \\delta t + \\psi (L) \\epsilon_t$\n  Wrong Treatment: subtracting $\\delta t$ from $y_t$ would succeed in removing the time-dependence of the mean but not the variance. For example, $ y_t - \\delta t = y_0 + (\\varepsilon_1 + \\varepsilon_2 + \\cdots + \\varepsilon_t) \\equiv y_0 + u_t $, where $Var(u_t) = t\\sigma^2$, time-varying variance.\n  thus subtracting a time trend from a unit root process is not sufficient to produce a stationary time series. Unless we know that $\\mu_t$ follows a covariance-stationary process.\n    Unit root process (if I(1), then random walk process): $ (1-L)y_t = \\sigma + \\psi(L) \\epsilon_t $ The alternative expression of this process is $y_t \\sim I(1)$, integrated of order 1. The term \u0026ldquo;integrated\u0026rdquo; comes from calculus; if $dy/dt = x$, then y is the integral of $x$. In discrete time series, if $\\Delta y_t = x_t $, then $y$ might be also viewed as the integral of $x$. Then this could extend to $ARIMA(p,d,q)$.\n  The Correct Treatment: difference the series to introduce into the moving average representation. $\\Delta y_t =\\sigma + (1-L)\\psi (L) \\varepsilon_t $\n  Log-difference and growth rate or percentage change: $$ \\begin{eqnarray*} (1-L)\\log(y_t) \u0026amp;=\u0026amp; \\log(y_t/y_{t-1}) \\nonumber \\\\\\\n\u0026amp;=\u0026amp; \\log { 1+ [(y_t - y_{t-1})/y_{t-1}]} \\nonumber \\\\\\\n\u0026amp;\\cong \u0026amp; (y_t - y_{t-1})/y_{t-1} \\end{eqnarray*} $$\n  Parsimonious models often perform best, and autoregressions are much easier to estimate and forecast than moving average processes, particularly moving average processes with a root near unity.\n  Cointegration  $y \\sim I(0)$ means stationary  $$ \\begin{eqnarray*} (1-\\Phi_1L -\\Phi_1L^2 -\\cdots -\\Phi_pL^p) y_t \u0026amp;=\u0026amp; (1+\\theta_1L +\\theta_2L^2 +\\cdots +\\theta_qL^q) \\varepsilon_t \\\\\\\nP(L) y_t \u0026amp;=\u0026amp; Q(L) \\varepsilon_t \\end{eqnarray*} $$\n if 1 eigen vaue of $P(L)$ is unity, all other are inside the unit circle, then $I(1)$. if 2 eigen vaues of $P(L)$ are unity, all other are inside the unit circle, then $I(2)$. if all roots outside unit circle in $P(L)$ then sationary. If all roots outside unit circle in $Q(L)$ then invertible. if invertible then $MA(q) \\to AR(\\infty)$. Then further $ARMA(p,q) \\to AR(\\infty)$. Dickey-Fuller unity root test. $H_0: y_t = y_{t-1} + c + \\varepsilon_t$. if it fails to reject, then non-stationary. Why stationarity is required? regeression is i.i.d, so it does not capture serial correlation.  ","id":2,"section":"notes","summary":"Reference links  PSU: https://online.stat.psu.edu/stat510/  Why does a time series have to be stationary? Stationarity is a one type of dependence structure.\nSuppose we have a data $X_1,\u0026hellip;,X_n$. The most basic assumption is that $X_i$ are independent, i.e. we have a sample. The independence is a nice property, since using it we can derive a lot of useful results. The problem is that sometimes (or frequently, depending on the view) this property does not hold.","tags":null,"title":"Time Series Notes","uri":"https://foxisawesome.github.io/notes/time_series_notes/","year":"2020"},{"content":"Reference: https://www.interviewbit.com/python-interview-questions/\n1. What is Python? Python is a high-level, interpreted, general-purpose programming language. Being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries. Additionally, python supports objects, modules, threads, exception-handling and automatic memory management which help in modelling real-world problems and building applications to solve these problems.\n2. What are the benefits of using Python? Python is a general-purpose programming language that has simple, easy-to-learn syntax which emphasizes readability and therefore reduces the cost of program maintenance. Moreover, the language is capable of scripting, completely open-source and supports third-party packages encouraging modularity and code-reuse. Its high-level data structures, combined with dynamic typing and dynamic binding, attract a huge community of developers for Rapid Application Development and deployment.\n3. What is a dynamically typed language? Before we understand what a dynamically typed language, we should learn about what typing is. Typing refers to type-checking in programming languages. In a *strongly-typed* language, such as Python, \u0026ldquo;1\u0026rdquo; + 2 will result in a type error, since these languages don\u0026rsquo;t allow for \u0026ldquo;type-coercion\u0026rdquo; (implicit conversion of data types). On the other hand, a *weakly-typed* language, such as Javascript, will simply output \u0026ldquo;12\u0026rdquo; as result.\nType-checking can be done at two stages -\n Static - Data Types are checked before execution. Dynamic - Data Types are checked during execution.  Python being an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution. Hence, Python is a Dynamically Typed language.\n4. What is an Interpreted language? An Interpreted language executes its statements line by line. Languages such as Python, Javascript, R, PHP and Ruby are prime examples of Interpreted languages. Programs written in an interpreted language runs directly from the source code, with no intermediary compilation step.\n5. What is PEP 8 and why is it important? PEP stands for Python Enhancement Proposal. A PEP is an official design document providing information to the Python Community, or describing a new feature for Python or its processes. PEP 8 is especially important since it documents the style guidelines for Python Code. Apparently contributing in the Python open-source community requires you to follow these style guidelines sincerely and strictly.\n6. How is memory managed in Python? Memory management in Python is handled by the Python Memory Manager. The memory allocated by the manager is in form of a private heap space dedicated for Python. All Python objects are stored in this heap and being private, it is inaccessible to the programmer. Though, python does provide some core API functions to work upon the private heap space. Additionally, Python has an in-built garbage collection to recycle the unused memory for the private heap space.\n7. What are Python namespaces? Why are they used? A namespace in Python ensures that object names in a program are unique and can be used without any conflict. Python implements these namespaces as dictionaries with \u0026lsquo;name as key\u0026rsquo; mapped to a corresponding \u0026lsquo;object as value\u0026rsquo;. This allows for multiple namespaces to use the same name and map it to a separate object. A few examples of namespaces are as follows:\n Local Namespace includes local names inside a function. the namespace is temporarily created for a function call and gets cleared when the function returns. Global Namespace includes names from various imported packages/ modules that is being used in the current project. This namespace is created when the package is imported in the script and lasts until the execution of the script. Built-in Namespace includes built-in functions of core Python and built-in names for various types of exceptions.  Lifecycle of a namespace depends upon the scope of objects they are mapped to. If the scope of an object ends, the lifecycle of that namespace comes to an end. Hence, it isn\u0026rsquo;t possible to access inner namespace objects from an outer namespace.\n8. What is Scope in Python? Every object in Python functions within a scope. A scope is a block of code where an object in Python remains relevant. Namespaces uniquely identify all the objects inside a program. However, these namespaces also have a scope defined for them where you could use their objects without any prefix. A few examples of scope created during code execution in Python are as follows:\n A local scope refers to the local objects available in the current function. A global scope refers to the objects available throught the code execution since their inception. A module-level scope refers to the global objects of the current module accessible in the program. An outermost scope refers to all the built-in names callable in the program. The objects in this scope are searched last to find the name referenced.  Note: Local scope objects can be synced with global scope objects using keywords such as global.\n9. What is Scope Resolution in Python? Sometimes objects within the same scope have the same name but function differently. In such cases, scope resolution comes into play in Python automatically. A few examples of such behaviour are:\n Python modules namely \u0026lsquo;math\u0026rsquo; and \u0026lsquo;cmath\u0026rsquo; have a lot of functions that are common to both of them - log10(), acos(), exp() etc. To resolve this amiguity, it is necessary to prefix them with their respective module, like math.exp() and cmath.exp(). Consider the code below, an object temp has been initialized to 10 globally and then to 20 on function call. However, the function call didn\u0026rsquo;t change the value of the temp globally. Here, we can observe that Python draws a clear line between global and local variables treating both their namespaces as separate identities.  temp = 10 # global-scope variable def func(): temp = 20 # local-scope variable print(temp) print(temp) # output =\u0026gt; 10 func() # output =\u0026gt; 20 print(temp) # output =\u0026gt; 10  This behaviour can be overriden using the global keyword inside the function, as shown in the following example:\ntemp = 10 # global-scope variable def func(): global temp temp = 20 # local-scope variable print(temp) print(temp) # output =\u0026gt; 10 func() # output =\u0026gt; 20 print(temp) # output =\u0026gt; 20  10. What are decorators in Python? Decorators in Python are essentially functions that add functionality to an existing function in Python without changing the structure of the function itself. They are represented by the @decorator_name in Python and are called in bottom-up fashion. For example:\n# decorator function to convert to lowercase def lowercase_decorator(function): def wrapper(): func = function() string_lowercase = func.lower() return string_lowercase return wrapper # decorator function to split words def splitter_decorator(function): def wrapper(): func = function() string_split = func.split() return string_split return wrapper @splitter_decorator\t# this is executed next @lowercase_decorator\t# this is executed first def hello(): return 'Hello World' hello() # output =\u0026gt; [ 'hello' , 'world' ]  The beauty of the decorators lies in the fact that besides adding functionality to the output of the method, they can even accept arguments for functions and can further modify those arguments before passing it to the function itself. The inner nested function, i.e. \u0026lsquo;wrapper\u0026rsquo; function, plays a significant role here. It is implemented to enforce encapsulation and thus, keep itself hidden from the global scope.\n# decorator function to capitalize names def names_decorator(function): def wrapper(arg1, arg2): arg1 = arg1.capitalize() arg2 = arg2.capitalize() string_hello = function(arg1, arg2) return string_hello return wrapper @names_decorator def say_hello(name1, name2): return 'Hello ' + name1 + '! Hello ' + name2 + '!' say_hello('sara', 'ansh') # output =\u0026gt; 'Hello Sara! Hello Ansh!'  11. What are lists and tuples? What is the key difference between the two? Lists and Tuples are both sequence data types that can store a collection of objects in Python. The objects stored in both sequences can have different data types. Lists are represented with square brackets ['sara', 6, 0.19], while tuples are represented with parantheses ('ansh', 5, 0.97). But what is the real difference between the two? The key difference between the two is that while lists are mutable, tuples on the other hand are immutable objects. This means that lists can be modified, appended or sliced on-the-go but tuples remain constant and cannot be modified in any manner. You can run the following example on Python IDLE to confirm the difference:\nmy_tuple = ('sara', 6, 5, 0.97) my_list = ['sara', 6, 5, 0.97] print(my_tuple[0]) # output =\u0026gt; 'sara' print(my_list[0]) # output =\u0026gt; 'sara' my_tuple[0] = 'ansh' # modifying tuple =\u0026gt; throws an error my_list[0] = 'ansh' # modifying list =\u0026gt; list modified print(my_tuple[0]) # output =\u0026gt; 'sara' print(my_list[0]) # output =\u0026gt; 'ansh'  12. What are Dict and List comprehensions? Python comprehensions, like decorators, are syntactic sugar constructs that help build altered and filtered lists, dictionaries or sets from a given list, dictionary or set. Using comprehensions, saves a lot of time and code that might be considerably more verbose (containing more lines of code). Let\u0026rsquo;s check out some examples, where comprehensions can be truly beneficial:\n  Performing mathematical operations on the entire list\nmy_list = [2, 3, 5, 7, 11] squared_list = [x**2 for x in my_list] # list comprehension # output =\u0026gt; [4 , 9 , 25 , 49 , 121] squared_dict = {x:x**2 for x in my_list} # dict comprehension # output =\u0026gt; {11: 121, 2: 4 , 3: 9 , 5: 25 , 7: 49}    Performing conditional filtering operations on the entire list\nmy_list = [2, 3, 5, 7, 11] squared_list = [x**2 for x in my_list if x%2 != 0] # list comprehension # output =\u0026gt; [9 , 25 , 49 , 121] squared_dict = {x:x**2 for x in my_list if x%2 != 0} # dict comprehension # output =\u0026gt; {11: 121, 3: 9 , 5: 25 , 7: 49}    Combining multiple lists into one\nComprehensions allow for multiple iterators and hence, can be used to combine multiple lists into one.\na = [1, 2, 3] b = [7, 8, 9] [(x + y) for (x,y) in zip(a,b)] # parallel iterators # output =\u0026gt; [8, 10, 12] [(x,y) for x in a for y in b] # nested iterators # output =\u0026gt; [(1, 7), (1, 8), (1, 9), (2, 7), (2, 8), (2, 9), (3, 7), (3, 8), (3, 9)]    Flattening a multi-dimensional list\nA similar approach of nested iterators (as above) can be applied to flatten a multi-dimensional list or work upon its inner elements.\nmy_list = [[10,20,30],[40,50,60],[70,80,90]] flattened = [x for temp in my_list for x in temp] # output =\u0026gt; [10, 20, 30, 40, 50, 60, 70, 80, 90]    Note: List comprehensions have the same effect as the map method in other languages. They follow the mathematical set builder notation rather than map and filter functions in Python.\n13. What are the common built-in data types in Python? There are several built-in data types in Python. Although, Python doesn\u0026rsquo;t require data types to be defined explicitly during variable declarations but type errors are likely to occur if the knowledge of data types and their compatibility with each other are neglected. Python provides type() and isinstance() functions to check the type of these variables. These data types can be grouped into the following catetgories-\n  None Type\nNone  keyword represents the null values in Python. Boolean equality operation can be performed using these NoneType objects.\n   Class Name Description     NoneType Represents the NULL values in Python      Numeric Types\n   Class Name Description     int Stores integer literals including hex, octal and binary numbers as integers   float Stores literals containing decimal values and/or exponent sign as floating-point numbers   complex Stores complex number in the form (A + Bj) and has attributes: real and imag   bool Stores boolean value (True or False)      Note: The standard library also includes fractions to store rational numbers and decimal to store floating-point numbers with user-defined precision.\n  Sequence Types\n   Class Name Description     list Mutable sequence used to store collection of items.   tuple Immutable sequence used to store collection of items.   range Represents an immutable sequence of numbers generated during execution.   str Immutable sequence of Unicode code points to store textual data.    Note: The standard library also includes additional types for processing:\n Binary data such as bytearray bytes memoryview , and Text strings such as str .    Mapping Type:\n   Class Name Description     dict Stores comma-separated list of key: value pairs      Set Types\n   Class Name Description     set Mutable unordered collection of distinct hashable objects   frozenset Immutable collection of distinct hashable objects      Note: set is mutable and thus cannot be used as key for a dictionary. On the other hand, frozenset is immutable and thus, hashable, and can be used as a dictionary key or as an element of another set.\n  Modules Module is an additional built-in type supported by the Python Interpreter. It supports one special operation, i.e., attribute access: mymod.myobj, where mymod is a module and myobj references a name defined in m\u0026rsquo;s symbol table. The module\u0026rsquo;s symbol table resides in a very special attribute of the module dict, but direct assignment to this module is neither possible nor recommended.\n  Callable Types Callable types are the types to which function call can be applied. They can be user-defined functions, instance methods, generator functions, and some other built-in functions, methods and classes. Refer the documentation at docs.python.org for a detailed view into the callable types.\n  14. What is lambda in Python? Why is it used? Lambda is an anonymous function in Python, that can accept any number of arguments, but can only have a single expression. It is generally used in situations requiring an anonymous function for a short time period. Lambda functions can be used in either of the two ways:\n  Assigning lambda functions to a variable\nmul = lambda a, b : a * b print(mul(2, 5)) # output =\u0026gt; 10    Wrapping lambda functions inside another function\ndef myWrapper(n): return lambda a : a * n mulFive = myWrapper(5) print(mulFive(2)) # output =\u0026gt; 10    15. What is pass in Python? The pass keyword represents a null operation in Python. It is generally used for the purpose of filling up empty blocks of code which may execute during runtime but has yet to be written. Without the pass statement in the following code, we may run into some errors during code execution.\ndef myEmptyFunc(): # do nothing pass myEmptyFunc() # nothing happens ## Without the pass keyword # File \u0026quot;\u0026lt;stdin\u0026gt;\u0026quot;, line 3 # IndentationError: expected an indented block  16. How do you copy an object in Python? In Python, the assignment statement (**=** operator) does not copy objects. Instead, it creates a binding between the existing object and the target variable name. To create copies of an object in Python, we need to use the copy module. Moreover, there are two ways of creating copies for the given object using the copy module -\n Shallow Copy is a bit-wise copy of an object. The copied object created has an exact copy of the values in the original object. If either of the values are references to other objects, just the reference addresses for the same are copied. Deep Copy copies all values recursively from source to target object, i.e. it even duplicates the objects referenced by the source object.  from copy import copy, deepcopy list_1 = [1, 2, [3, 5], 4] ## shallow copy list_2 = copy(list_1) list_2[3] = 7 list_2[2].append(6) list_2 # output =\u0026gt; [1, 2, [3, 5, 6], 7] list_1 # output =\u0026gt; [1, 2, [3, 5, 6], 4] ## deep copy list_3 = deepcopy(list_1) list_3[3] = 8 list_3[2].append(7) list_3 # output =\u0026gt; [1, 2, [3, 5, 6, 7], 8] list_1 # output =\u0026gt; [1, 2, [3, 5, 6], 4]  18. What are modules and packages in Python? Python packages and Python modules are two mechanisms that allow for modular programming in Python. Modularizing ahs several advantages -\n Simplicity: Working on a single module helps you focus on a relatively small portion of the problem at hand. This makes development easier and less error-prone. Maintainability: Modules are designed to enforce logical boundaries between different problem domains. If they are written in a manner that reduces interdependency, it is less likely that modifications in a module might impact other parts of the program. Reusability: Functions defined in a module can be easily reused by other parts of the application. Scoping: Modules typically define a separate namespace, which helps avoid confusion between identifiers from other parts of the program.  Modules, in general, are simply Python files with a .py extension and can have a set of functions, classes or variables defined and implemented. They can be imported and initialized once using the import statement. If partial functionality is needed, import the requisite classes or functions using from foo import bar.\nPackages allow for hierarchial structuring of the module namespace using dot notation. As, modules help avoid clashes between global variable names, in a similary manner, packages help avoid clashes between module names. Creating a package is easy since it makes use of the system\u0026rsquo;s inherent file structure. So just stuff the modules into a folder and there you have it, the folder name as the package name. Importing a module or its contents from this package requires the package name as prefix to the module name joined by a dot.\nNote: You can technically import the package as well, but alas, it doesn\u0026rsquo;t import the modules within the package to the local namespace, thus, it is practically useless.\n19. What are global, protected and private attributes in Python?  Global variables are public variables that are defined in the global scope. To use the variable in the global scope inside a function, we use the global keyword. Protected attributes are attributes defined with a underscore prefixed to their identifier eg. _sara. They can still be accessed and modified from outside the class they are defined in but a responsible developer should refrain from doing so. Private attributes are attributes with double underscore prefixed to their identifier eg. __ansh. They cannot be accessed or modified from the outside directly and will result in an AttributeError if such an attempt is made.  20. What is self in Python? Self is a keyword in Python used to define an instance or an object of a class. In Python, it is explicity used as the first paramter, unlike in Java where it is optional. It helps in disinguishing between the methods and attributes of a class from its local variables.\n21. What is init? __init__ is a contructor method in Python and is automatically called to allocate memory when a new object/instance is created. All classes have a init method associated with them. It helps in distinguishing methods and attributes of a class from local variables.\n# class definition class Student: def __init__(self, fname, lname, age, section): self.firstname = fname self.lastname = lname self.age = age self.section = section # creating a new object stu1 = Student(\u0026quot;Sara\u0026quot;, \u0026quot;Ansh\u0026quot;, 22, \u0026quot;A2\u0026quot;)  22. What is break, continue and pass in Python?    Break The break statement terminates the loop immediately and the control flows to the statement after the body of the loop.     Continue The continue statement terminates the current iteration of the statement, skips the rest of the code in the current iteration and the control flows to the next iteration of the loop.   Pass As explained above, pass keyword in Python is generally used to fill-up empty blocks and is similar to an empty statement represented by a semi-colon in languages such as Java, C++, Javascript etc.    pat = [1, 3, 2, 1, 2, 3, 1, 0, 1, 3] for p in pat: pass if (p == 0): current = p break elif (p % 2 == 0): continue print(p) # output =\u0026gt; 1 3 1 3 1 print(current) # output =\u0026gt; 0  23. What is pickling and unpickling? Python library offers a feature - serialization out of the box. Serializing a object refers to transforming it into a format that can be stored, so as to be able to deserialize it later on, to obtain the original object. Here, the pickle module comes into play.\nPickling Pickling is the name of the serialization process in Python. Any object in Python can be serialized into a byte stream and dumped as a file in the memory. The process of pickling is compact but pickle objects can be compressed further. Moreover, pickle keeps track of the objects it has serialized and the serialization is portable across versions. The function used for the above process is pickle.dump().\nUnpickling Unpickling is the complete inverse of pickling. It deserializes the byte stream to recreate the objects stored in the file, and loads the object to memory. The function used for the above process is pickle.load().\nNote: Python has another, more primitive, serialization module called marshall, which exists primarily to support .pyc files in Python and differs significantly from pickle.\n24. What are generators in Python? Generators are functions that return an iterable collection of items, one at a time, in a set manner. Generators, in general, are used to create iterators with a different approach. They employ the use of yield keyword rather than return to return a generator object. Let\u0026rsquo;s try and build a generator for fibonacci numbers -\n## generate fibonacci numbers upto n def fib(n): p, q = 0, 1 while(p \u0026lt; n): yield p p, q = q, p + q x = fib(10) # create generator object ## iterating using __next__() x.__next__() # output =\u0026gt; 0 x.__next__() # output =\u0026gt; 1 x.__next__() # output =\u0026gt; 1 x.__next__() # output =\u0026gt; 2 x.__next__() # output =\u0026gt; 3 x.__next__() # output =\u0026gt; 5 x.__next__() # output =\u0026gt; 8 x.__next__() # error ## iterating using loop for i in fib(10): print(i) # output =\u0026gt; 0 1 1 2 3 5 8  25. What is PYTHONPATH in Python? PYTHONPATH is an environment variable which you can set to add additional directories where Python will look for modules and packages. This is especially useful in maintaining Python libraries that you do not wish to install in the global default location.\n26. What is the use of help() and dir() functions? help() function in Python is used to display the documentation of modules, classes, functions, keywords, etc. If no parameter is passed to the help() function, then an interactive help utility is launched on the console. dir() function tries to return a valid list of attributes and methods of the object it is called upon. It behaves differently with different objects, as it aims to produce the most relevant data, rather than the complete information.\n For Modules/Library objects, it returns a list of all attributes, contained in that module. For Class Objects, it returns a list of all valid attributes and base attributes. With no arguments passed, it returns a list of attributes in the current scope.  27. What is the difference between .py and .pyc files?  .py files contain the source code of a program. Whereas, .pyc file contains the bytecode of your program. We get bytecode after compilation of .py file (source code). .pyc files are not created for all the files that you run. It is only created for the files that you import. Before executing a python program python interpreter checks for the compiled files. If the file is present, the virtual machine executes it. If not found, it checks for .py file. If found, compiles it to .pyc file and then python virtual machine executes it. Having .pyc file saves you the compilation time.  28. How Python is interpreted?  Python as a language is not interpreted or compiled. Interpreted or compiled is the property of the implementation. Python is a bytecode(set of interpreter readable instructions) interpreted generally. Source code is a file with .py extension. Python compiles the source code to a set of instructions for a virtual machine. The Python interpreter is an implementation of that virtual machine. This intermediate format is called “bytecode”. .py source code is first compiled to give .pyc which is bytecode. This bytecode can be then interpreted by official CPython, or JIT(Just in Time compiler) compiled by PyPy.  29. What are unittests in Python?  unittest is a unit testing framework of Python. Unit testing means testing different components of software separately. Can you think why unit testing is important? Imagine a scenario, you are building software which uses three components namely A, B, and C. Now, suppose your software breaks at a point time. How will you find which component was responsible for breaking the software? Maybe it was component A that failed, which in turn failed component B, and this actually failed the software. There can be many such combinations. This is why it is necessary to test each and every component properly so that we know which component might be highly responsible for the failure of the software.  30. What is docstring in Python?  Documentation string or docstring is a multiline string used to document a specific code segment. The docstring should describe what the function or method does.  31. How are arguments passed by value or by reference in python?  Pass by value: Copy of the actual object is passed. Changing the value of the copy of the object will not change the value of the original object. Pass by reference: Reference to the actual object is passed. Changing the value of the new object will change the value of the original object.  In Python, arguments are passed by reference, i.e., reference to the actual object is passed.\ndef appendNumber(arr): arr.append(4) arr = [1, 2, 3] print(arr) #Output: =\u0026gt; [1, 2, 3] appendNumber(arr) print(arr) #Output: =\u0026gt; [1, 2, 3, 4]  32. What are iterators in Python?  Iterator is an object. It remembers its state i.e., where it is during iteration (see code below to see how) iter() method initializes an iterator. It has a next() method which returns the next item in iteration and points to the next element. Upon reaching the end of iterable object next() must return StopIteration exception. It is also self iterable. Iterators are objects with which we can iterate over iterable objects like lists, strings, etc.  class ArrayList: def __init__(self, number_list): self.numbers = number_list def __iter__(self): self.pos = 0 return self def __next__(self): if(self.pos \u0026lt; len(self.numbers)): self.pos += 1 return self.numbers[self.pos - 1] else: raise StopIteration array_obj = ArrayList([1, 2, 3]) it = array_obj.__iter__() # it = iter(array_obj) print(it.__next__()) #output: 1 print(it.__next__()) #output: 2 print(it.__next__()) #output: 3 print(next(it)) #Throws Exception #Traceback (most recent call last): #... #StopIteration  33. What is slicing in Python?  As the name suggests, ‘slicing’ is taking parts of. Syntax for slicing is [start : stop : step] start is the starting index from where to slice a list or tuple stop is the ending index or where to sop. step is the number of steps to jump. Default value for start is 0, stop is number of items, step is 1. Slicing can be done on strings, arrays, lists, and tuples.  numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] print(numbers[1 : : 2]) #output : [2, 4, 6, 8, 10]  34. Explain how can you make a Python Script executable on Unix?  Script file must begin with #!/usr/bin/env python  35. Explain how to delete a file in Python?  Use command os.remove(file_name)  import os os.remove(\u0026quot;ChangedFile.csv\u0026quot;) print(\u0026quot;File Removed!\u0026quot;)  36. Explain split() and join() functions in Python?  You can use split() function to split a string based on a delimiter to a list of strings. You can use join() function to join a list of strings based on a delimiter to give a single string.  string = \u0026quot;This is a string.\u0026quot; string_list = string.split(' ') #delimiter is ‘space’ character or ‘ ‘ print(string_list) #output: ['This', 'is', 'a', 'string.'] print(' '.join(string_list)) #output: This is a string.  37. What is the difference between Python Arrays and lists?  Arrays in python can only contain elements of same data types i.e., data type of array should be homogeneous. It is a thin wrapper around C language arrays and consumes far less memory than lists. Lists in python can contain elements of different data types i.e., data type of lists can be heterogeneous. It has the disadvantage of consuming large memory.  import array a = array.array('i', [1, 2, 3]) for i in a: print(i, end=' ') #OUTPUT: 1 2 3 a = array.array('i', [1, 2, 'string']) #OUTPUT: TypeError: an integer is required (got type str) a = [1, 2, 'string'] for i in a: print(i, end=' ') #OUTPUT: 1 2 string  38. What does *args and **kwargs mean? *args\n *args is a special syntax used in function definition to pass variable-length argument. “*” means variable length and “args” is the name used by convention. You can use any other.  def multiply(a, b, *argv): mul = a * b for num in argv: mul *= num return mul print(multiply(1, 2, 3, 4, 5)) #output: 120  **kwargs\n **kwargs is a special syntax used in function definition to pass variable-length keyworded argument. Here, also, “kwargs” is used just by convention. You can use any other name. Keyworded argument means a variable which has a name when passed to a function. It is actually a dictionary of variable name and its value.  def tellArguments(**kwargs): for key, value in kwargs.items(): print(key + \u0026quot;: \u0026quot; + value) tellArguments(arg1 = \u0026quot;argument 1\u0026quot;, arg2 = \u0026quot;argument 2\u0026quot;, arg3 = \u0026quot;argument 3\u0026quot;) #output: # arg1: argument 1 # arg2: argument 2 # arg3: argument 3  39. What are negative indexes and why are they used?  Negative indexes are the indexes from the end of the list or tuple or string. Arr[-1] means last element of array Arr[]  arr = [1, 2, 3, 4, 5, 6] #get the last element print(arr[-1]) #output 6 #get the second last element print(arr[-2]) #output 5  ","id":3,"section":"notes","summary":"Reference: https://www.interviewbit.com/python-interview-questions/\n1. What is Python? Python is a high-level, interpreted, general-purpose programming language. Being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries. Additionally, python supports objects, modules, threads, exception-handling and automatic memory management which help in modelling real-world problems and building applications to solve these problems.\n2. What are the benefits of using Python? Python is a general-purpose programming language that has simple, easy-to-learn syntax which emphasizes readability and therefore reduces the cost of program maintenance.","tags":null,"title":"Python Basics Notes","uri":"https://foxisawesome.github.io/notes/python_basics/","year":"2020"},{"content":" [[x,y] for x in [0,3] for y in [6,9]]: generate pairs [[0, 6], [0, 9], [3, 6], [3, 9]] [[x,y] for x, y in zip(['a','b'],['c','d'])]: [[\u0026lsquo;a\u0026rsquo;, \u0026lsquo;c\u0026rsquo;], [\u0026lsquo;b\u0026rsquo;, \u0026rsquo;d']] one liner to create dictionary:lookup = {val: i for i, val in enumerate(order)} this reverse check dict and return max val count in dict  max_val = max(dict.values()) max_freq = max(dict, key=dict.get) for key, val in dict.items(): if val == max_val: return key sorted_dict = sorted(dict, key = lambda x: (-dict[x]) ) sorted_dict[0]   sort a by b: [x for _,x in sorted(zip(b,a))]  ","id":4,"section":"notes","summary":" [[x,y] for x in [0,3] for y in [6,9]]: generate pairs [[0, 6], [0, 9], [3, 6], [3, 9]] [[x,y] for x, y in zip(['a','b'],['c','d'])]: [[\u0026lsquo;a\u0026rsquo;, \u0026lsquo;c\u0026rsquo;], [\u0026lsquo;b\u0026rsquo;, \u0026rsquo;d']] one liner to create dictionary:lookup = {val: i for i, val in enumerate(order)} this reverse check dict and return max val count in dict  max_val = max(dict.values()) max_freq = max(dict, key=dict.get) for key, val in dict.items(): if val == max_val: return key sorted_dict = sorted(dict, key = lambda x: (-dict[x]) ) sorted_dict[0]   sort a by b: [x for _,x in sorted(zip(b,a))]  ","tags":null,"title":"Cheatsheet","uri":"https://foxisawesome.github.io/notes/cheatsheet/","year":"2020"},{"content":"滑动窗口算法的思路是这样：\n1、我们在字符串S中使用双指针中的左右指针技巧，初始化left = right = 0，把索引左闭右开区间[left, right)称为一个「窗口」。\n2、我们先不断地增加right指针扩大窗口[left, right)，直到窗口中的字符串符合要求（包含了T中的所有字符）。\n3、此时，我们停止增加right，转而不断增加left指针缩小窗口[left, right)，直到窗口中的字符串不再符合要求（不包含T中的所有字符了）。同时，每次增加left，我们都要更新一轮结果。\n4、重复第 2 和第 3 步，直到right到达字符串S的尽头。\n这个思路其实也不难，第 2 步相当于在寻找一个「可行解」，然后第 3 步在优化这个「可行解」，最终找到最优解，也就是最短的覆盖子串。左右指针轮流前进，窗口大小增增减减，窗口不断向右滑动，这就是「滑动窗口」这个名字的来历。\nrefrence: wx:labuladong\nCode Template:\ndef slidingWindow(s: str, t: str) -\u0026gt; str: target, window = {}, {} for i in t: target[i] = target.get(i,0) + 1 left, right, valid = 0, 0, 0 while (right \u0026lt; len(s)): # expand window right point to contain all t c = s[right] # move right index right += 1 # 进行窗口内数据的一系列更新 ... # /*** debug 输出的位置 ***/ print('s: '+s2[left:right]+' window' + str(window) + ' valid: ' + str(valid)) # see if needs to shrink window while (window shrinking condition): d = s[left] left += 1 # 进行窗口内数据的一系列更新 ... return ...  ","id":5,"section":"notes","summary":"滑动窗口算法的思路是这样： 1、我们在字符串S中使用双指针中的左右指针技巧，初始化left = right = 0，把索引左闭右开区间[left, right)","tags":["template"],"title":"Sliding Window Note","uri":"https://foxisawesome.github.io/notes/sliding-window-note/","year":"2020"},{"content":"开始刷Leetcode 好几个月了，今天开始学很多人用blog 记录下自己的notes，也激励自己好好刷题！First post!\nIt takes a while for me to get how exactly Linked List works. I used following utility functions to keep my leetcode solutions and run codes locally.\nNote:\n say head = ListNode(0), then head = head.next is to iterate to next node on the Linked List. use head.next = a to point to next node.  # My liked list utility functions: class ListNode: def __init__(self, val=0, next=None): self.val = val self.next = next def list2ListNode(list): head = LinkedList = ListNode(0) for i in range(len(list)): LinkedList.next = ListNode(list[i]) LinkedList = LinkedList.next return head.next def printListNode(p): head = ListNode(0) head.next = p out = [] while p: out.append(p.val) p = p.next return out # e.g.: indata = [1,2,3,4,5,6] head = ListNode.list2ListNode(indata) ListNode.printListNode(head)  ","id":6,"section":"notes","summary":"开始刷Leetcode 好几个月了，今天开始学很多人用blog 记录下自己的notes，也激励自己好好刷题！First post! It takes a while for me to get how exactly Linked List","tags":["template","linkedList"],"title":"Linked List Note (First Post)","uri":"https://foxisawesome.github.io/notes/linked-list-note/","year":"2020"},{"content":"reference: The 5 Most Useful Techniques to Handle Imbalanced Datasets\n1. Random Undersampling and Oversampling A widely adopted and perhaps the most straightforward method for dealing with highly imbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling).\nLet us first create some example imbalanced data.\nfrom sklearn.datasets import make_classification X, y = make_classification( n_classes=2, class_sep=1.5, weights=[0.9, 0.1], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=100, random_state=10 ) X = pd.DataFrame(X) X['target'] = y  We can now do random oversampling and undersampling using:\nnum_0 = len(X[X['target']==0]) num_1 = len(X[X['target']==1]) print(num_0,num_1) # random undersample undersampled_data = pd.concat([ X[X['target']==0].sample(num_1) , X[X['target']==1] ]) print(len(undersampled_data)) # random oversample oversampled_data = pd.concat([ X[X['target']==0] , X[X['target']==1].sample(num_0, replace=True) ]) print(len(oversampled_data)) ----------------------------- OUTPUT: 90 10 20 180  2. Undersampling and Oversampling using imbalanced-learn imbalanced-learn(imblearn) is a Python Package to tackle the curse of imbalanced datasets.\nIt provides a variety of methods to undersample and oversample.\na. Undersampling using Tomek Links: One of such methods it provides is called Tomek Links. Tomek links are pairs of examples of opposite classes in close vicinity.\nIn this algorithm, we end up removing the majority element from the Tomek link, which provides a better decision boundary for a classifier.\nfrom imblearn.under_sampling import TomekLinks tl = TomekLinks(return_indices=True, ratio='majority') X_tl, y_tl, id_tl = tl.fit_sample(X, y)  b. Oversampling using SMOTE: In SMOTE (Synthetic Minority Oversampling Technique) we synthesize elements for the minority class, in the vicinity of already existing elements.\nfrom imblearn.over_sampling import SMOTE smote = SMOTE(ratio='minority') X_sm, y_sm = smote.fit_sample(X, y)  There are a variety of other methods in the imblearn package for both undersampling(Cluster Centroids, NearMiss, etc.) and oversampling(ADASYN and bSMOTE) that you can check out.\n3. Class weights in the models Most of the machine learning models provide a parameter called class_weights. For example, in a random forest classifier using, class_weights we can specify a higher weight for the minority class using a dictionary.\nfrom sklearn.linear_model import LogisticRegression clf = LogisticRegression(class_weight={0:1,1:10})  But what happens exactly in the background?\nIn logistic Regression, we calculate loss per example using binary cross-entropy:\nLoss = −ylog(p) − (1−y)log(1−p)  In this particular form, we give equal weight to both the positive and the negative classes. When we set class_weight as class_weight = {0:1,1:20}, the classifier in the background tries to minimize:\nNewLoss = −20*ylog(p) − 1*(1−y)log(1−p)  So what happens exactly here?\n If our model gives a probability of 0.3 and we misclassify a positive example, the NewLoss acquires a value of $-20\\log(0.3) = 10.45$ If our model gives a probability of 0.7 and we misclassify a negative example, the NewLoss acquires a value of $-\\log(0.3) = 0.52$  That means we penalize our model around twenty times more when it misclassifies a positive minority example in this case.\nHow can we compute class_weights?\nThere is no one method to do this, and this should be constructed as a hyperparameter search problem for your particular problem.\nBut if you want to get class_weights using the distribution of the y variable, you can use the following nifty utility from sklearn.\nfrom sklearn.utils.class_weight import compute_class_weight class_weights = compute_class_weight('balanced', np.unique(y), y)  4. Change your Evaluation Metric Choosing the right evaluation metric is pretty essential whenever we work with imbalanced datasets. Generally, in such cases, the F1 Score is what I want as my evaluation metric.\nThe F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.\nSo how does it help?\nLet us start with a binary prediction problem. We are predicting if an asteroid will hit the earth or not.\nSo we create a model that predicts “No” for the whole training set.\nWhat is the accuracy(Normally the most used evaluation metric)?\nIt is more than 99%, and so according to accuracy, this model is pretty good, but it is worthless.\nNow, what is the F1 score?\nOur precision here is 0. What is the recall of our positive class? It is zero. And hence the F1 score is also 0.\nAnd thus we get to know that the classifier that has an accuracy of 99% is worthless for our case. And hence it solves our problem.\nPrecision-Recall Tradeoff\nSimply stated the F1 score sort of maintains a balance between the precision and recall for your classifier. If your precision is low, the F1 is low, and if the recall is low again, your F1 score is low.\n If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.\n How to Use? You can calculate the F1 score for binary prediction problems using:\nfrom sklearn.metrics import f1_score y_true = [0, 1, 1, 0, 1, 1] y_pred = [0, 0, 1, 0, 0, 1] f1_score(y_true, y_pred)  This is one of my functions which I use to get the best threshold for maximizing F1 score for binary predictions. The below function iterates through possible threshold values to find the one that gives the best F1 score.\n# y_pred is an array of predictions def bestThresshold(y_true,y_pred): best_thresh = None best_score = 0 for thresh in np.arange(0.1, 0.501, 0.01): score = f1_score(y_true, np.array(y_pred)\u0026gt;thresh) if score \u0026gt; best_score: best_thresh = thresh best_score = score return best_score , best_thresh  ","id":7,"section":"notes","summary":"reference: The 5 Most Useful Techniques to Handle Imbalanced Datasets\n1. Random Undersampling and Oversampling A widely adopted and perhaps the most straightforward method for dealing with highly imbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling).\nLet us first create some example imbalanced data.\nfrom sklearn.datasets import make_classification X, y = make_classification( n_classes=2, class_sep=1.5, weights=[0.","tags":null,"title":"Imbalance Data Handling Notes","uri":"https://foxisawesome.github.io/notes/imbalance_data_handling_notes/","year":"2019"}],"tags":[{"title":"array","uri":"https://foxisawesome.github.io/tags/array/"},{"title":"backtracking","uri":"https://foxisawesome.github.io/tags/backtracking/"},{"title":"BFS","uri":"https://foxisawesome.github.io/tags/bfs/"},{"title":"binary_search","uri":"https://foxisawesome.github.io/tags/binary_search/"},{"title":"BST","uri":"https://foxisawesome.github.io/tags/bst/"},{"title":"DFS","uri":"https://foxisawesome.github.io/tags/dfs/"},{"title":"dia","uri":"https://foxisawesome.github.io/tags/dia/"},{"title":"dia_sql","uri":"https://foxisawesome.github.io/tags/dia_sql/"},{"title":"dp","uri":"https://foxisawesome.github.io/tags/dp/"},{"title":"GRAPH","uri":"https://foxisawesome.github.io/tags/graph/"},{"title":"greedy","uri":"https://foxisawesome.github.io/tags/greedy/"},{"title":"hashtable","uri":"https://foxisawesome.github.io/tags/hashtable/"},{"title":"heap","uri":"https://foxisawesome.github.io/tags/heap/"},{"title":"leetcode","uri":"https://foxisawesome.github.io/tags/leetcode/"},{"title":"linkedList","uri":"https://foxisawesome.github.io/tags/linkedlist/"},{"title":"math","uri":"https://foxisawesome.github.io/tags/math/"},{"title":"matrix","uri":"https://foxisawesome.github.io/tags/matrix/"},{"title":"monotonic","uri":"https://foxisawesome.github.io/tags/monotonic/"},{"title":"recursion","uri":"https://foxisawesome.github.io/tags/recursion/"},{"title":"sliding_window","uri":"https://foxisawesome.github.io/tags/sliding_window/"},{"title":"sort","uri":"https://foxisawesome.github.io/tags/sort/"},{"title":"stack","uri":"https://foxisawesome.github.io/tags/stack/"},{"title":"string","uri":"https://foxisawesome.github.io/tags/string/"},{"title":"template","uri":"https://foxisawesome.github.io/tags/template/"},{"title":"tree","uri":"https://foxisawesome.github.io/tags/tree/"},{"title":"two_pointers","uri":"https://foxisawesome.github.io/tags/two_pointers/"}]}